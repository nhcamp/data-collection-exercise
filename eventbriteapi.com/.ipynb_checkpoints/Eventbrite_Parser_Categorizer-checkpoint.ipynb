{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_key= \"insert key here'\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind my classification system is building a labeled training set to train a probabilistic classifier that will eventually be able to take titles and or descriptions of events WITHOUT LABELS and correctly predict their category. I think this solution is quite viable because of the nature of event titles and descriptions. For instance, the presence of the word \"Vs\" will almost always be attached to a sporting event. Additionally the beauty of using a probabilisitic classifier is that we can classify events into an \"Other\" category if there is high ambiguity. Human review of these events into their appropriate category and inclusion into model training will ensure improved model performance with successive iterations.\n",
    "\n",
    "I think a great place to start in building a labeled training set, particularly where EventBrite events are concerned, is using their classification system. Putting in a call to their API allows us to get back the categories and subcategories they associate with each id number. Personally, I think this list is a little bit long. I could see using an abbreivated version, which I have given an example of below. This is of personal preference, as I don't generally like having more than ~12-15 choices in a dropdown menu. Using a system of dictionaries will let us easily transcribe labels across APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = requests.get(\"https://www.eventbriteapi.com/v3/categories/?token=\" + private_key).json()\n",
    "sub_categories = requests.get(\"https://www.eventbriteapi.com/v3/subcategories/?token=\" + private_key).json()\n",
    "\n",
    "activities_list = ['Music','Visual Arts','Performing Arts','Film','Lectures and Books','Fashion','Food and Drink',\n",
    "'Festivals and Fairs','Charities','Sports', 'Active Life','Nightlife','Kids and Family','Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I'll take the JSON of categories and subcategories from above and make dictionaries of id:name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = {}\n",
    "for cat in categories['categories']:\n",
    "    id_name[cat['id']] = cat['name']\n",
    "    \n",
    "id_subname = {}\n",
    "for sub_cat in sub_categories['subcategories']:\n",
    "    id_subname[sub_cat['id']] = sub_cat['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll open the JSON file pulled from your repo and make a list of events as requested. Flattening the nested structure, pulling the event name, event description, and categories into a dateframe, and mapping category names to ids will leave us with a structure ideal for training a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "with open(\"eb-response-page1.json\", 'r') as activities:\n",
    "    data = json.load(activities)\n",
    "    for each in data['events']:\n",
    "        events.append(each)\n",
    "    activities.close()\n",
    "    \n",
    "df = pd.io.json.json_normalize(events)\n",
    "df = df[['name.text','description.text','id','category_id','subcategory_id']]\n",
    "df['category_classified'] = df.category_id.map(id_name)\n",
    "df['sub_category_classified'] = df.subcategory_id.map(id_subname)\n",
    "\n",
    "df['description.text'] = df['description.text'].str.replace('\\n',' ') \n",
    "df['description.text'] = df['description.text'].str.lower()                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple steps sbove we're ready to vectorize our text. Here I've used TfidfVectorizer with logarithmic term frequency, minimum document frequency of 2 (which would be adjusted based one size of training set), both uni and bigrams, and stopwords removed. Fitting this to the description gives us 435 features for our 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 435)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['category_classified']\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2',\n",
    "                        ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df['description.text']).toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use a Multinomial Naive Bayes in this simple example but I think it's a reasonable choice without having tested other probabilisitic classifiers. As mentioned above, we can use a threshold probability to assign events to an \"Other\" category until we can improve certainty with future training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB().fit(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also wanted to look at the ticketmaster API call to see that this method will still work. The output of the TM API is a crazy nested structure but the same general principle works. Developing pre-processing pipelines for each API will be the most important part behind this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_events = []\n",
    "with open('tm-response-page1-Copy1.json', 'r') as ticketmaster:\n",
    "    data = json.load(ticketmaster)\n",
    "    for each in data['_embedded']['events']:\n",
    "        info = [each['name'],each['classifications'][0]['segment']]\n",
    "        tm_events.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Portland Trail Blazers vs. Los Angeles Lakers',\n",
       " {'id': 'KZFzniwnSyZfZ7v7nE', 'name': 'Sports'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly I'll output a CSV of event id, name and category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df[['name.text','id','category_id']]\n",
    "output.to_csv('eventbrite_events', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
